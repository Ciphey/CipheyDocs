%\documentclass[twocolumn]{article}
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{gensymb}
\usepackage{geometry}
\usepackage{enumitem}
\graphicspath{{.}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}{Algorithm}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}

\title{AuSearch}
\author{Harlan Connor}

\begin{document}
\maketitle
\pagebreak

\section{Problem}
In Ciphey, we need to navigate a tree of unbounded depth, trying to find a 
solution node. Each node falls into the following types:
\begin{enumerate}[label=\Roman*]
\item Has zero or more children (a confirmable cipher)
\item Has one child which is definitely a solution, or no children (an inferable cipher)
\item  Has zero or more children, and can be done in neligible time (an encoding)
\end{enumerate}

Evaluating the children of a node is an intensive task, and so is checking nodes,
so we wish to minimise both actions. We must find such a node if there is any 
reasonable chance that it exists (above a threshold $\ell_p$), and if it can be 
found in reasonable time $\ell_t$.

To aid us in our quest, we have the following oracles for any node $A$:
\begin{itemize}
\item A metric that tells us either (for type I) the rough likelihood of there being any children ($p_A$), or (for type II) the rough likelihood of this being the final node ($P_A$); 
      the probability of a successful crack
\item The average running time of finding the children if there are some ($t_A$);
      the average runtime of a successful crack
\item The average running time of finding out that there are no children ($T_A$);
      the average running time of an unsuccessful crack
\end{itemize}

If we let $\omega_A$ be the time taken to run if $A$ is on the fastest route,
$\Omega_A$ be the time taken if it isn't, and $\tau_A$ be the time taken to examine the rest of the tree.
We can arrive at a rather simplistic optimal weight system right away:
\[
	W(A) = p_A \cdot (t_A + (1-P_A) (\omega_A + \tau_A)) + (1 - p_A) \cdot (T_A + \tau_A)
\]

Needless to say, $\tau_A$ is uncomputable without evaluating every node, as a
trivial result of the undecidability of the Halting Problem. $\omega_A$ and $\Omega_A$ have similar issues.

In this paper, we set about trying to find a replacement heuristic for $\tau_A$, as well as trying to approximate $\omega_A$ and $\Omega_A$.

We will use $\xi$ to represent the root node.

\section{Modelling}
%We will assume that all ciphers that are successfully cracked either yield the plaintext 
%directly, or are the ancestor of the plaintext. Even though this is not \emph{always}
%the case, we will be able to navigate out of such situations (at a time penalty).

We assume that there is no overhead in getting these stats. This is again untrue,
but if we require that any non-trivial work is done in the actual gathering of 
children, at the expense of weakening the heuristic.

We assume that type I nodes will have a probability of being the solution $P$ high
enough that we can treat it as 1 in the weight calculation. However, this is tweakable within the algorithm, so I will just leave $P$ as-is.

We also assume that there are no identical nodes. This is achieved in Ciphey by
filtering out reoccurring values, which causes reality to move away from the
heuristic a small bit.

We can assume that a successful crack means that we are on the right path.

%\section{Method}
%Early solutions did not keep track of what was below them, leading to a sunk cost
%fallacy, where the program would just recurse down the most efficient path at each 
%point.
%
%When discussing the problem with Brandon Skerritt, he briefly suggested Alpha-Beta 
%pruning and minimax style solutions to the problem. Thinking on this theme, I came upon
%this chess engine-like solution.
%
%Let's first examine a basic non-pruning chess engine. It has a tree of nodes, each node representing a
%position on the board. We can either heuristically evaluate it (how many pieces do we have,
%are they in strong positions), or we can look at the child nodes, evaluate them, and use the
%information to calculate a more informed evaluation of the position.
%
%This replacement of a single heuristic with a refined combination of heuristics only works because
%a position is only as good as where it gets you: an ideal engine would look at all the possible
%future positions that could arise, and work off that.
%
%Luckily for us, our problem can be thought of in a not dissimilar way.
%
%RANTS
%
%Each node $A$, when evaluated in some order $N$, either contains or does not contain
%the solution.
%
%We can evaluate the expected run time of a child as the expected run time of the root.
%
%We use the simplification that $\tau_\xi = \Omega_\xi$
%
%For type II nodes, $\omega_A = \Omega_A = 0$, and $p_A = P_A$, so $W(A) = (1-P_A)(T_A + \tau_A)$, a massive simplification!
%
%This isn't a tree, its a list of possible results!
%
%We do not care about the runtime if it is not a member of $L$
%
%We don't care about infinite recursion, as the heuristic now perfectly aims for
%what we want!
%
%	 Find some list $N$ formed of all of the elements of $L$,
%	      where each element only occurs once, which minimises 
%	      $p_{N_0}(t_{N_0} + \omega_{N_0}) + (1-p_{N_0})(p_{N_1}(t_{N_1} + \omega_{N_0}) + (1-p_{N_1})(...))$, which represents the average time taken if the solution is a member of $L$.
%	 Since, the probability that none of the nodes are the solution 
%	      $\prod_{i \in L} (1-P_L)$ is usually very small, we can approximate 
%	      $\omega_\xi$ as that value. 
%	 $\Omega_\xi$ could be infinite, but using the same assumption, we
%	      can approimate it as $\sum_{i \in N}(1 - p_i)(T_i + \Omega_i)$
%	      
%	      We dont care if type I or type II
%	      
%	      Time can be in any linear unit, so it doesn't matter if we measure on one machine
%
%	We assume $\forall A: \omega_A = \omega_\lambda \land \Omega_A = \Omega_\lambda $

\section{Proposed Solution}
Instead of storing the results as the intuitive tree data structure, it makes more
sense to think about it as a list, as we do not care what the parent of a node is!

We let Info($A$ : Node) be the function that gives us the information about the 
node, and Evaluate($A$ : Node) be the function that gives us the result of 
procesing.

We can gather $\omega_\xi$ and $\Omega_\xi$ through sampling for each type 
of data, and use them as an approximation for any $\omega_A$ and $\Omega_A$ that
we may encounter of that type.

\begin{algorithm}[FindBestOrder($S$ : List[Node])]\,\\
We need to find the node $A$ which has the lowest value of:
\[
	W(A) =  p_A \cdot t_A + (1 - p_A) \cdot (T_A + W(N))
\]

	Where $N$ is some a sequence formed from unique elements of $S \setminus \{A\}$, that minimises $W(A)$:
\[ 
	W(N) = 
	\begin{cases}
		0 & \text{if}\,|n| = 0 \\
		p_{N_0} t_{N_0} + (1 - p_{N_0}) (T_{N_0} + W(N')) & \text{otherwise}
	\end{cases}
\]

RETURN $<A, N>$
\end{algorithm}

\begin{algorithm}[CipheySearch(\textit{CText} : data)]\,\\
\begin{enumerate}
\item Let the set $L = \text{Evaluate}(<$Info(\textit{CText})$, [\,]>$
\item If \textit{CText} was the solution, return it.
\item Create our node pointer $A$
\item WHILE $|L| \neq 0$
	\begin{enumerate}
	\item Expand all encodings to their fullest depth
	\item If we have been running for more than $\ell_t$, ERROR "Timeout".
	\item $<A, N> := $ CALL FindBestOrder$(L)$
	\item Derive $p_N$ from $P_{[\,]} = 0$, $p_N := p_{N_0} + (1-p_{N_0})p_{N'}$
	\item If $p_N := p_{N_0} + (1-p_{N_0})p_{N'}$ is less than $\ell_p$, ERROR "Unlikely".
	\item Remove $A$ from $L$
	\item $S := \text{Evaluate}(A)$
	\item If it is the solution, RETURN $A$
	\item If it has no children, CONTINUE
	\item $L := L \cup \text{Info}(S)$
	\end{enumerate}
\item ERROR "List exhausted"
\end{enumerate}
\end{algorithm}

\section{The Algorithm Implementation}

\subsection{Assumptions}

\begin{itemize}
\item We know exactly what is going into the lists. Because the lists will only container crackers and decryptors
\item We know exactly how many items will be in the list, because we know what is going into it
\item the only thing we do not know is the specific ordering of the list based on the weight function
\item We are implementing this as a regular Python list
\item Nothing we do not know will be added to the list. I.E. The list is deterministic as a multi-set. It will only contain items we know, and will always be a specific length or an easily calculable length.
\end{itemize}

\subsection{Data Structure Ideas}
We can use a Complete Binary Tree. This data structure has O(log n) search and O(log n) pop / deletion. And because it is a \textbf{complete} binary tree, it can be implemented as a regular Python list.

More specifically, we will implement this tree as a Heap. A heap is a data structure that uses a CBT (Complete Binary Tree) as its backend. Most importantly, the first first element [0] will always be the smallest element.

Here are some more facts about heaps:
\begin{itemize}
	\item The first element will always be the smallest
	\item The value of a node is always smaller than its children
	\item For every node, its first child is at $2*k + 1$, its second child is at $2*k + 2$, and its parent is at $floor((k + 1) / 2)$.
	\item If H is a heap, then the following will never be false $
h[k] <= h[2*k + 1] and h[k] <= h[2*k + 2]$. 
\item HeapReplace is equivalent to heapPop and heapPush. 
\item HeapPushPop is equivalent to heapPush followed by heapPop
\item The Python heap module implements merge() which is used for merging sorted sequences into the heap.
\end{itemize}

\subsection{Ideas}
Each decryptor / cracker will return their values as heaps.

Since we have a families "encoding, basicEncryption, hashes" we perform Merge on these values.
Since our assumption is that we know \textbf{exactly}, what each function is returning, and they are deterministic we can simply do insertion sort --- but without the O(n) check to see if it is sorted. We do this check on non-deterministic inputs. But since we know the exact contents of the return, we do not need to perform this check. We simply insert.

When we merge, we also know the exact contents of the input to the merge. This means that again, we do not need to stress too much about how to effectively merge 2 things if we already know what the functions are. All we have to do is make sure we maintain the heap property.

At the end, we will have a merged list which follows the heap property.

So the idea is that instead of taking the time to turn the list into a heap everytime, we simply build the heap as we take the inputs from each node. And since we already know what the heap looked like before, and we know everything apart from the minimise value, we can use some clever CompSci to minimise the amount of time needed to Heapify the list.

Once we have a heap, naturally the first element will always be the lowest. 

But this is assuming we need to evaluate the entire list every time. 

I am thinking that the weight changes will not be significant. I.E. $80$ cannot become $2$. The weight changes will most likely be smaller than that change.

My thinking is this:
* We have a list of weights, $\omega_A$
* We perform the crackers / dectyptors, and store the result in $A$
* We now have list of weights $\omega_B$. Our list now looks like $[\omega_A, \omega_B]$.
\textbf{Path 1}. Normally we would recalculate every single weight, giving us an exponential growth rate.
\textbf{Path 2}. Assuming that the weights do not massively change, we only re-calculate the weights we select.
* For this algorithm, choose part 2. So we store all the new weights, and we hasify the list with both the old weights, and the new weights. If the element at index 0 is an old weight, we re-calculate it. If it is still the lowest, we use it. If not, we go to the 2nd lowest.

\subsubsection{Issues with the above algorithm}
* It may be possible for an old weight at #3 to be updated to the new lowest weight, which means the algorithm isn't optimal.
* If the weights change every time, and they are relient on the previous weight, we will have to expend space complexity to store all weights until we update them.
\end{document}
