%\documentclass[twocolumn]{article}
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{enumerate}
\usepackage{gensymb}
\usepackage{geometry}
\usepackage{enumitem}
\graphicspath{{.}}

% Credit: https://tex.stackexchange.com/a/95841/
\usepackage{tabularx}
\newenvironment{conditions*}
  {\par\vspace{\abovedisplayskip}\noindent
   \tabularx{\columnwidth}{>{$}l<{$} @{${}={}$} >{\raggedright\arraybackslash}X}}
  {\endtabularx\par\vspace{\belowdisplayskip}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}{Algorithm}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}

\title{AuSearch}
\author{Harlan Connor}

\begin{document}
\maketitle
\pagebreak

\section{Problem}
In Ciphey, we need to navigate a tree of unbounded depth, trying to find a 
solution node. Each node falls into the following types:
\begin{enumerate}[label=\Roman*]
\item Has zero or more children (a confirmable cipher)
\item Has one child which is definitely a solution, or no children (an inferable cipher)
\item  Has zero or more children, and can be done in neligible time (an encoding)
\end{enumerate}

Evaluating the children of a node is an intensive task, and so is checking nodes,
so we wish to minimise both actions. We must find such a node if there is any 
reasonable chance that it exists (above a threshold $\ell_p$), and if it can be 
found in reasonable time $\ell_t$.

To aid us in our quest, we have the following oracles for any node $A$:
\begin{itemize}
\item A metric ($p_A$) that tells us either (for type I) the rough likelihood of there being any children, or (for type II) the rough likelihood of this being the final node (with $P_A$, the likelihood of this being the correct node given that we cracked it, being $1$); 
      the probability of a successful crack
\item The average running time of finding the children if there are some ($t_A$);
      the average runtime of a successful crack
\item The average running time of finding out that there are no children ($T_A$);
      the average running time of an unsuccessful crack
\end{itemize}

An optimal heuristic (if we could compute it instantly), would be:
\[
	W(A) = p_A (t_A + c_A + (1-P_A) (\omega_A + (1-k_A) \tau_A)) + (1 - p_A) (T_A + \tau_A)
\]
where
\begin{conditions*}
	c_A & the time taken to check $A$ \\
	\omega_A & the time taken to run if $A$ is on the fastest route \\
	\tau_A & the time taken to evaluate the tree barring $A$ and its children \\
	k_A & the likelihood that $A$ is on the correct path \\
\end{conditions*}

Needless to say, $\tau_A$ is uncomputable without evaluating every node, as a
trivial result of the undecidability of the Halting Problem. $\omega_A$ and $k_A$
don't look particularly promising either.

In this paper, we set about trying to find a replacement heuristic for $\tau_A$, and for
methods of approximating $\omega_A$ and $k_A$.

\section{Modelling}
We assume that there is no overhead in getting these stats. This is again untrue,
but if we require that any non-trivial work is done in the actual gathering of 
children, at the expense of increasing the uncertainty in the heuristic.

We also assume that there are no two nodes with the same child (i.e. this is a tree). This is achieved in Ciphey by filtering out reoccurring values.

We can assume that a successful crack means that we are on the right path ($k_A = 1$),
this allows us to unify type I and type II nodes. For most ciphers, the chance of this
assumption failing is astronomically small.

This last assumption allows us to simplify the weight calculation:
\begin{align*}
W(A) &\approx p_A (t_A + c_A + (1-P_A) (\omega_A + (1-1) \tau_A)) + (1 - p_A) (T_A + \tau_A) \\
     &= p_A (t_A + c_A + (1-P_A) \omega_A) + (1 - p_A) (T_A + \tau_A)
\end{align*}

Since nested ciphers are rather rare, we can assume they do not contribute a meaningful
amount to the weight ($P_A \approx 1$):

\begin{align*}
W(A) &\approx p_A (t_A + c_A + (1-1) \omega_A + (1 - p_A) (T_A + \tau_A) \\
     &= p_A (t_A + c_A) + (1 - p_A) (T_A + \tau_A)
\end{align*}
\section{Proposed Solution}
Instead of storing the results as a tree (which is the immediate reaction most would 
have), it makes more sense to think about it as a list, as we do not care what the
parent of a node is!

We let Info($A$ : Node) be the function that gives us the information about the 
node, and Evaluate($A$ : Node) be the function that gives us the result of 
processing.

I will use $N'$ to mean the tail (the sequence formed by removing the first element) of
the sequence $N$

\begin{algorithm}[FindBestOrder($S$ : List(Node)]\,\\
TODO: Get someone who understands NLP to do this for me

We need to find the node $A$ which has the lowest value of:
\[
	W(A) =  p_A (t_A + c_A) + (1 - p_A) (T_A + W(N))
\]

	Where $N$ is some a sequence formed from unique elements of $S \setminus \{A\}$, that minimises $W(A)$:
\[ 
	W(N) = 
	\begin{cases}
		0 & \text{if}\,|n| = 0 \\
		p_{N_0} (t_{N_0}+ c_{N_0}) + (1 - p_{N_0}) (T_{N_0} + W(N')) & \text{otherwise}
	\end{cases}
\]

RETURN $<A, N>$
\end{algorithm}

\begin{algorithm}[AuSearch(\textit{CText} : data)]\,\\
\begin{enumerate}
\item Let the set $L = \text{Evaluate}(<$Info(\textit{CText})$, [\,]>$
\item If \textit{CText} was the solution, return it.
\item Create our node pointer $A$
\item WHILE $|L| \neq 0$
	\begin{enumerate}
	\item Expand all encodings to their fullest depth
	\item If we have been running for more than $\ell_t$, ERROR "Timeout".
	\item $<A, N> := $ CALL FindBestOrder$(L)$
	\item Derive $p_S = p_A + (1-p_A) p_N$ from $p_{[\,]} = 0$, $p_N := p_{N_0} + (1-p_{N_0})p_{N'}$
	\item If $p_N := p_{N_0} + (1-p_{N_0})p_{N'}$ is less than $\ell_p$, ERROR "Unlikely".
	\item Remove $A$ from $L$
	\item $S := \text{Evaluate}(A)$
	\item If it is the solution, RETURN $A$
	\item If it has no children, CONTINUE
	\item $L := L \cup \text{Info}(S)$
	\end{enumerate}
\item ERROR "List exhausted"
\end{enumerate}
\end{algorithm}

\section{The Algorithm Implementation}

\subsection{Assumptions}

\begin{itemize}
\item We know exactly what is going into the lists. Because the lists will only container crackers and decryptors
\item We know exactly how many items will be in the list, because we know what is going into it
\item the only thing we do not know is the specific ordering of the list based on the weight function
\item We are implementing this as a regular Python list
\item Nothing we do not know will be added to the list. I.E. The list is deterministic as a multi-set. It will only contain items we know, and will always be a specific length or an easily calculable length.
\end{itemize}

\subsection{Data Structure Ideas}
We can use a Complete Binary Tree. This data structure has O(log n) search and O(log n) pop / deletion. And because it is a \textbf{complete} binary tree, it can be implemented as a regular Python list.

More specifically, we will implement this tree as a Heap. A heap is a data structure that uses a CBT (Complete Binary Tree) as its backend. Most importantly, the first first element [0] will always be the smallest element.

Here are some more facts about heaps:
\begin{itemize}
	\item The first element will always be the smallest
	\item The value of a node is always smaller than its children
	\item For every node, its first child is at $2*k + 1$, its second child is at $2*k + 2$, and its parent is at $floor((k + 1) / 2)$.
	\item If H is a heap, then the following will never be false $
h[k] <= h[2*k + 1] and h[k] <= h[2*k + 2]$. 
\item HeapReplace is equivalent to heapPop and heapPush. 
\item HeapPushPop is equivalent to heapPush followed by heapPop
\item The Python heap module implements merge() which is used for merging sorted sequences into the heap.
\end{itemize}

\subsection{Ideas}
Each decryptor / cracker will return their values as heaps.

Since we have a families "encoding, basicEncryption, hashes" we perform Merge on these values.
Since our assumption is that we know \textbf{exactly}, what each function is returning, and they are deterministic we can simply do insertion sort --- but without the O(n) check to see if it is sorted. We do this check on non-deterministic inputs. But since we know the exact contents of the return, we do not need to perform this check. We simply insert.

When we merge, we also know the exact contents of the input to the merge. This means that again, we do not need to stress too much about how to effectively merge 2 things if we already know what the functions are. All we have to do is make sure we maintain the heap property.

At the end, we will have a merged list which follows the heap property.

So the idea is that instead of taking the time to turn the list into a heap everytime, we simply build the heap as we take the inputs from each node. And since we already know what the heap looked like before, and we know everything apart from the minimise value, we can use some clever CompSci to minimise the amount of time needed to Heapify the list.

Once we have a heap, naturally the first element will always be the lowest. 

But this is assuming we need to evaluate the entire list every time. 
\end{document}
